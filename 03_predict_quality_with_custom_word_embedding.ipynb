{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predict the quality of Wine based on the description with custom Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and explore data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/winemag-data-130k-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# split points into binary label (80-89 = bad, 90-99 = good)\n",
    "df['label'] = df['points'].apply(lambda x: 'good' if x > 89 else 'bad')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from util import cleanse_data\n",
    "\n",
    "clean_txt = cleanse_data(df)\n",
    "df['clean_desc'] = clean_txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for col in df.clean_desc:\n",
    "    word_list = col.split(' ')\n",
    "    corpus.append(word_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word embedding has a vocabulary size of 30463 words.\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "print(f'The word embedding has a vocabulary size of {len(model.wv)} words.')\n",
    "\n",
    "model.save('embeddings\\description_emb.bin')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['clean_desc']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6779380650125024\n"
     ]
    }
   ],
   "source": [
    "from util import train_bernoulli\n",
    "\n",
    "model_min1_vs100 = train_bernoulli('embeddings\\description_emb.bin', X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So we have an accuracy of around 67%. This is worse than the tfidf vectorizer. We have multiple approaches to fix this. We could try to finetune the parameters of our word embedding, we could use other prebuilt word embeddings or we could also use other models than the BernoulliNB. Since the word embedding that we just built was rather simple we should start by improving on it first."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Let's create word embeddings with different parameters\n",
    "\n",
    "model_min_count_two_100 = gensim.models.Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=2, workers=4)\n",
    "model_min_count_two_100.save('embeddings\\description_emb_min2_vs100.bin')\n",
    "\n",
    "model_min_count_three_100 = gensim.models.Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=3, workers=4)\n",
    "model_min_count_three_100.save('embeddings\\description_emb_min3_vs100.bin')\n",
    "\n",
    "model_min_count_one_300 = gensim.models.Word2Vec(sentences=corpus, vector_size=300, window=5, min_count=1, workers=4)\n",
    "model_min_count_one_300.save('embeddings\\description_emb_min1_vs300.bin')\n",
    "\n",
    "model_min_count_two_300 = gensim.models.Word2Vec(sentences=corpus, vector_size=300, window=5, min_count=2, workers=4)\n",
    "model_min_count_two_300.save('embeddings\\description_emb_min2_vs300.bin')\n",
    "\n",
    "model_min_count_three_300 = gensim.models.Word2Vec(sentences=corpus, vector_size=300, window=5, min_count=3, workers=4)\n",
    "model_min_count_three_300.save('embeddings\\description_emb_min3_vs300.bin')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6874783612233122\n"
     ]
    }
   ],
   "source": [
    "model_min2_vs100 = train_bernoulli('embeddings\\description_emb_min2_vs100.bin', X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6847855356799385\n"
     ]
    }
   ],
   "source": [
    "model_min3_vs100 = train_bernoulli('embeddings\\description_emb_min3_vs100.bin', X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6748220811694556\n"
     ]
    }
   ],
   "source": [
    "model_min1_vs300 = train_bernoulli('embeddings\\description_emb_min1_vs300.bin', X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6829390267359108\n"
     ]
    }
   ],
   "source": [
    "model_min2_vs300 = train_bernoulli('embeddings\\description_emb_min2_vs300.bin', X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.676860934795153\n"
     ]
    }
   ],
   "source": [
    "model_min3_vs300 = train_bernoulli('embeddings\\description_emb_min3_vs300.bin', X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: The results seem to vary a little, even with a set random state for the data split. However, the differences are not huge.\n",
    "\n",
    "The best performing word embedding seems to be the one with min_count=2 and vector_size=300. It's accuracy is around 69% which still is way worse than the tfidf vectorizer.\n",
    "It seems like we cannot improve this any further with our own word embedding. Let's test some prebuilt word embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}